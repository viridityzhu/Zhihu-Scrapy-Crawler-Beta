# Scrapy知乎爬虫

爬知乎话题页、话题页用户、用户粉丝和关注者、用户信息。

## 首先是对爬虫的情况的简单描述

我们这次重新写了一份爬虫，对之前的框架进行了简化，主要是取消了分布式爬取，改成了本地爬取的方式，因为这样方便追查爬虫情况，也可以避开之前遇到的云服务器端口的漏洞

我们这次爬虫的进步主要是解决了数据缺失的问题，首先是采用log文件，把爬取的历史记录都保存下来，方便追溯数据缺失的情况；再就是我们对访问请求被暂时忽略的情况进行了额外的处理，就是存下来之后换ip再重爬，这样就保证了数据的完整

采用临时数据记录主要是为了让爬虫可以多次运行，还有就是3个人分工

最后就是我们租用了代理ip来绕过知乎封ip的反爬机制，这个费用是30元一天

## 第二方面是对数据和速度情况的简单说明

我们这次重新找了两个活跃的话题，一个是专业的，一个是非专业的，每个话题找到了一千一百多个种子用户，然后分别爬这些种子用户的粉丝和关注者，对大V进行了抽样，这样一个话题页大概有几十万个粉丝和几万个关注者。这所有用户的个人信息大概有118万人。

这样的话，爬虫每周就需要爬大约250万次。

关于爬虫的速度，我们本周进行了第一次测试，但因为中间遇到代码bug，有停下来调试，耽误了一些时间，所以速度不太好估算。但大体上，如果按最后的最好的状况来估算的话，爬一期的数据理论上只需要30个小时，所以一周一期的速度应该是没有问题的。
